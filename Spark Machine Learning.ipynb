{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.linalg.Vectors\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "denseVec: org.apache.spark.ml.linalg.Vector = [1.0,2.0,3.0]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val denseVec = Vectors.dense(1.0,2.0,3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVec: org.apache.spark.ml.linalg.Vector = (10,[1,2,3,4,5],[11.0,12.0,13.0,14.0,15.0])\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val SparseVec= Vectors.sparse(10,Array(1,2,3,4,5),Array(11,12,13,14,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark ML Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature\r\n",
       "import org.apache.spark.sql.functions._\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [id: bigint, date: string ... 19 more fields]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.option(\"header\",\"true\").option(\"delimiter\",\",\").option(\"inferSchema\",\"true\")\n",
    "    .csv(\"C:\\\\Users\\\\vitormendes\\\\Documents\\\\Coursera\\\\Untitled Folder\\\\Week 2\\\\home_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| price|\n",
      "+------+\n",
      "|221900|\n",
      "|538000|\n",
      "+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select($\"price\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [price: double]\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 = df.select($\"price\".cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "priceAssembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_a509675bc62e\r\n",
       "res31: priceAssembler.type = vecAssembler_a509675bc62e\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val priceAssembler = new feature.VectorAssembler()\n",
    "priceAssembler.setInputCols(Array(\"price\")).setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "priceAssemblerDF: org.apache.spark.sql.DataFrame = [price: int, features: vector]\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val priceAssemblerDF = priceAssembler.transform(df.select($\"price\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scaler_minmax: org.apache.spark.ml.feature.MinMaxScaler = minMaxScal_c3d6e87337b7\r\n",
       "res38: scaler_minmax.type = minMaxScal_c3d6e87337b7\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scaler_minmax = new feature.MinMaxScaler().setMin(1).setMax(50)\n",
    "scaler_minmax.setInputCol(\"features\").setOutputCol(\"price_feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scaled: org.apache.spark.sql.DataFrame = [price: int, features: vector ... 1 more field]\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scaled = scaler_minmax.fit(priceAssemblerDF).transform(priceAssemblerDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scaled2: org.apache.spark.sql.DataFrame = [price: int, features: vector ... 2 more fields]\n"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scaled2 = scaled.select(\n",
    "    expr(\"*\"),\n",
    "    when($\"price\" > 200000 , $\"price\").otherwise(null).cast(\"double\").alias(\"a\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(price)|\n",
      "+-----------------+\n",
      "|540088.1419053348|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaled2.agg(mean($\"price\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "imputer: org.apache.spark.ml.feature.Imputer = imputer_a2024ae49650\r\n",
       "res74: imputer.type = imputer_a2024ae49650\n"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val imputer = new feature.Imputer()\n",
    "imputer\n",
    "    .setInputCols(Array(\"a\"))\n",
    "    .setOutputCols(Array(\"b\"))\n",
    "    .setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scaled3: org.apache.spark.sql.DataFrame = [price: int, features: vector ... 3 more fields]\n"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scaled3 = imputer.fit(scaled2).transform(scaled2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+-----------------+\n",
      "| price|  features|       price_feature|       a|                b|\n",
      "+------+----------+--------------------+--------+-----------------+\n",
      "|221900|[221900.0]|[1.9440131147540982]|221900.0|         221900.0|\n",
      "|538000|[538000.0]| [3.975344262295082]|538000.0|         538000.0|\n",
      "|180000|[180000.0]|[1.6747540983606557]|    null|555145.9780473715|\n",
      "|604000|[604000.0]| [4.399475409836066]|604000.0|         604000.0|\n",
      "|510000|[510000.0]| [3.795409836065574]|510000.0|         510000.0|\n",
      "+------+----------+--------------------+--------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaled3.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R Formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "students: org.apache.spark.sql.DataFrame = [id: int, name: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val students = Seq(\n",
    "    (1,\"Vitor\",\"tall\",9.0),\n",
    "    (2,\"Henrique\",\"tall\",8.7),\n",
    "    (3,\"Akira\",\"short\",5.5),\n",
    "    (1,\"Vitor\",\"short\",6.2)\n",
    ").toDF(\"id\",\"name\",\"size\",\"grade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+-----+\n",
      "| id|    name| size|grade|\n",
      "+---+--------+-----+-----+\n",
      "|  1|   Vitor| tall|  9.0|\n",
      "|  2|Henrique| tall|  8.7|\n",
      "|  3|   Akira|short|  5.5|\n",
      "|  1|   Vitor|short|  6.2|\n",
      "+---+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "formula: org.apache.spark.ml.feature.RFormula = RFormula(grade ~ size) (uid=rFormula_11ebaf888f83)\r\n",
       "res81: formula.type = RFormula(grade ~ size) (uid=rFormula_11ebaf888f83)\n"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val formula = new feature.RFormula()\n",
    "formula.setFormula(\"grade ~ size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+-----+--------+-----+\n",
      "| id|    name| size|grade|features|label|\n",
      "+---+--------+-----+-----+--------+-----+\n",
      "|  1|   Vitor| tall|  9.0|   [1.0]|  9.0|\n",
      "|  2|Henrique| tall|  8.7|   [1.0]|  8.7|\n",
      "|  3|   Akira|short|  5.5|   [0.0]|  5.5|\n",
      "+---+--------+-----+-----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "formula.fit(students).transform(students).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import feature.{StringIndexer, OneHotEncoder}\n"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feature.{StringIndexer,OneHotEncoder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "string_indexer: org.apache.spark.sql.DataFrame = [id: int, name: string ... 3 more fields]\r\n",
       "hot_encoder: org.apache.spark.ml.feature.OneHotEncoder = oneHot_519d0993cb71\n"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val string_indexer = new StringIndexer().setInputCol(\"size\").setOutputCol(\"size_indexer\").fit(students).transform(students)\n",
    "val hot_encoder = new OneHotEncoder().setInputCol(\"size_indexer\").setOutputCol(\"size_binarization\").setDropLast(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "students_ml: org.apache.spark.sql.DataFrame = [id: int, name: string ... 4 more fields]\n"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val students_ml = hot_encoder.transform(string_indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+-----+------------+-----------------+\n",
      "| id|    name| size|grade|size_indexer|size_binarization|\n",
      "+---+--------+-----+-----+------------+-----------------+\n",
      "|  1|   Vitor| tall|  9.0|         1.0|    (2,[1],[1.0])|\n",
      "|  2|Henrique| tall|  8.7|         1.0|    (2,[1],[1.0])|\n",
      "|  3|   Akira|short|  5.5|         0.0|    (2,[0],[1.0])|\n",
      "|  1|   Vitor|short|  6.2|         0.0|    (2,[0],[1.0])|\n",
      "+---+--------+-----+-----+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_ml.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-------------+----------+\n",
      "|    name| id|         size|     grade|\n",
      "+--------+---+-------------+----------+\n",
      "|Henrique|  2|       [tall]|     [8.7]|\n",
      "|   Akira|  3|      [short]|     [5.5]|\n",
      "|   Vitor|  1|[short, tall]|[9.0, 6.2]|\n",
      "+--------+---+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students.groupBy(\"name\").agg(\n",
    "    max($\"id\").alias(\"id\"),collect_set($\"size\").alias(\"size\"),collect_set($\"grade\").alias(\"grade\")\n",
    ").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
